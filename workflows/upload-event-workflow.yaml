  # Execute the Google Cloud Storage (GCS) Object Uploaded Event
  #
  #
  main:
    params: [event]
    steps:
        - log_event:
            call: sys.log
            args:
                text: ${event}
                severity: INFO
        - check_content_type:
            switch:
                - condition: ${not text.match_regex(event.data.contentType, "video") and not text.match_regex(event.data.contentType, "octet-stream")}
                  return: ${"Media " + event.data.contentType + " not valid."}
                  next: end
            next: assign_backend_service
        - assign_backend_service:
            try:
              assign:
                - backend_service: ${event.backend_service}
            except:
              assign:
                - backend_service: ""
            next: assign_vars
        - assign_vars:
            switch:
              - condition: ${backend_service != ""} # Defined by bulk-upload
                assign:
                  - BACKEND_SRV: ${backend_service}
                  - OUTPUT_PATH: ${backend_service + "/" + sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
                  - SPLIT_FILE_NAME: ${text.split(event.data.name, "/")}
                  - OUTPUT_FILE_NAME: ${SPLIT_FILE_NAME[len(SPLIT_FILE_NAME) - 1]}
              - condition: ${text.match_regex(event.data.name, "transcoderapi/")} # GCS file path
                assign:
                  - BACKEND_SRV: "transcoderapi"
                  - OUTPUT_PATH: ${"transcoderapi/" + sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
                  - SPLIT_FILE_NAME: ${text.split(event.data.name, "/")}
                  - OUTPUT_FILE_NAME: ${SPLIT_FILE_NAME[len(SPLIT_FILE_NAME) - 1]}
              - condition: ${text.match_regex(event.data.name, "batchapi/")} # GCS file path
                assign:
                  - BACKEND_SRV: "batchapi"
                  - OUTPUT_PATH: ${"batchapi/" + sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
              - condition: true # Default
                assign:
                  - BACKEND_SRV: "gke"
                  - OUTPUT_PATH: ${"gke/" + sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
            next: bq_job_insert
        - bq_job_insert:
            call: googleapis.bigquery.v2.tabledata.insertAll
            args:
              projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
              datasetId: ${sys.get_env("BQ_JOBS_STATS_DATASET")}
              tableId: ${sys.get_env("BQ_JOBS_STATS_TABLE")}
              body: # TableDataInsertAllRequest
                rows:
                  - json:
                        JobId: ${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
                        createdDateTime: ${text.split(time.format(sys.now(), "UTC"), "Z")[0]}
                        BackendSrv: ${BACKEND_SRV}
                        fileURI: ${"gs://" + event.bucket + "/" + event.data.name}
                        contentType: ${event.data.contentType}
            next: prep_job
        - prep_job:
            assign:
                - project: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                - location: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                - jobId: '${"transcoding-" + sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}'
                - imageUri: ${sys.get_env("DOCKER_IMAGE_URI")}
                - gcsDestination: ${sys.get_env("GCS_DESTINATION")}
                - gkeClusterName: ${sys.get_env("GKE_CLUSTER_NAME")}
                - gkeNamespace: ${sys.get_env("GKE_NAMESPACE")}
            next: create_output_folder
        - create_output_folder:
            call: googleapis.storage.v1.objects.insert
            args:
              bucket: ${gcsDestination}
              name: ${OUTPUT_PATH + "/stats.txt"}
            next: decide_backend_svc
        - decide_backend_svc:
            switch:
              - condition: ${BACKEND_SRV == "transcoderapi"}
                call: schedule_transcoder_job
                args:
                    JOB_ID: ${jobId}
                    BUCKET_NAME: ${event.bucket}
                    FILE_NAME: ${event.data.name}
                    PROJECT: ${project}
                    LOCATION: ${location}
                    OUTPUT_BUCKET_NAME: ${gcsDestination}
                    OUTPUT_FILE_NAME: ${OUTPUT_FILE_NAME}
                    OUTPUT_PATH: ${OUTPUT_PATH}
                result: transcoderjob
              - condition: ${BACKEND_SRV == "batchapi"}
                call: schedule_batch_job
                args:
                    JOB_ID: ${jobId}
                    BUCKET_NAME: ${event.bucket}
                    FILE_NAME: ${event.data.name}
                    PROJECT: ${project}
                    LOCATION: ${location}
                    OUTPUT_BUCKET_NAME: ${gcsDestination}
                    OUTPUT_PATH: ${OUTPUT_PATH}
                    CONTAINER_IMAGE_URL: ${imageUri}
                result: batchjob
              - condition: ${BACKEND_SRV == "gke"}
                call: schedule_k8s_job
                args:
                    JOB_ID: ${jobId}
                    BUCKET_NAME: ${event.bucket}
                    FILE_NAME: ${event.data.name}
                    PROJECT: ${project}
                    LOCATION: ${location}
                    OUTPUT_BUCKET_NAME: ${gcsDestination}
                    OUTPUT_PATH: ${OUTPUT_PATH}
                    CONTAINER_IMAGE_URL: ${imageUri}
                    GKE_CLUSTER_NAME: ${gkeClusterName}
                    GKE_NAMESPACE: ${gkeNamespace}
                result: gkejob
        - finish_workflow:
            switch:
              - condition: ${BACKEND_SRV == "transcoderapi"}
                return: ${transcoderjob}
              - condition: ${BACKEND_SRV == "batchapi"}
                return: ${batchjob}
              - condition: ${BACKEND_SRV == "gke"}
                return: ${gkejob}


  # Call the Transcoder API to process the video file
  #
  #
  schedule_transcoder_job:
    params: [JOB_ID, BUCKET_NAME, FILE_NAME, PROJECT, LOCATION, OUTPUT_BUCKET_NAME, OUTPUT_FILE_NAME, OUTPUT_PATH]
    steps:
        - call_transcoder_api:
            call: googleapis.transcoder.v1.projects.locations.jobs.create
            args:
                parent: ${"projects/" + PROJECT + "/locations/" + LOCATION}
                body:
                  name: "${JOB_ID}"
                  config:
                    inputs:
                      - key: ${FILE_NAME}
                        uri: ${"gs://" + BUCKET_NAME + "/" + FILE_NAME}
                    elementaryStreams:
                      - key: "video-stream0"
                        videoStream: 
                          h264:
                            widthPixels: 640
                            heightPixels: 360
                            frameRate: 30
                            bitrateBps: 550000
                            pixelFormat: "yuv420p"
                            rateControlMode: "vbr"
                            crfLevel: 21
                            gopDuration: "3s"
                            vbvSizeBits: 550000
                            vbvFullnessBits: 495000
                            entropyCoder: "cabac"
                            bFrameCount: 3
                            aqStrength: 1
                            profile: "high"
                            preset: "veryfast"
                      - key: "audio-stream0"
                        audioStream:
                          codec: "aac"
                          bitrateBps: 64000
                          channelCount: 2
                          channelLayout:
                            - "fl"
                            - "fr"
                          sampleRateHertz: 48000
                    muxStreams:
                      - key: "sd"
                        fileName: ${OUTPUT_FILE_NAME}
                        container: "mp4"
                        elementaryStreams:
                          - "video-stream0"
                          - "audio-stream0"
                    output:
                      uri: ${"gs://" + OUTPUT_BUCKET_NAME + "/" + OUTPUT_PATH + "/"}
            result: transcoderjob
        - result:
            return: ${transcoderjob}

  # Establish a Batch Compute Engine (GCE) Job to process the video file
  #
  #
  schedule_batch_job:
    params: [JOB_ID, BUCKET_NAME, FILE_NAME, PROJECT, LOCATION, OUTPUT_BUCKET_NAME, OUTPUT_PATH, CONTAINER_IMAGE_URL]
    steps:
        - call_batch_api:
            call: googleapis.batch.v1.projects.locations.jobs.create
            args:
                parent: ${"projects/" + PROJECT + "/locations/" + LOCATION}
                jobId: "${JOB_ID}"
                body:
                    priority: 99
                    taskGroups:
                    - taskCount: 1
                      parallelism: 1

                      taskSpec:
                        computeResource:
                          cpuMilli: ${sys.get_env("MACHINE_CPU_MILLI")}
                          memoryMib: ${sys.get_env("MACHINE_MEMORY_MIB")}
                        runnables:
                        - container:
                            imageUri: "${CONTAINER_IMAGE_URL}"
                            entrypoint: ''
                            volumes:
                            - "/mnt/disks/output:/output"
                            - "/mnt/disks/input:/input"
                        environment:
                            variables:
                              MEDIA: "${FILE_NAME}"
                              OUTPUT_PATH: "/"
                        volumes:
                        - gcs:
                            remotePath: "${BUCKET_NAME}"
                          mountPath: "/mnt/disks/input"
                        - gcs:
                            remotePath: ${OUTPUT_BUCKET_NAME + "/" + OUTPUT_PATH}
                          mountPath: "/mnt/disks/output"
                    allocationPolicy:
                      serviceAccount:
                        email: ${sys.get_env("GOOGLE_CLOUD_PROJECT_NUMBER") + "-compute@developer.gserviceaccount.com"}
                        scopes: ["https://www.googleapis.com/auth/cloud-platform"]
                      network:
                        networkInterfaces:
                          - network: ${sys.get_env("VPC_NETWORK_FULLNAME")}
                            subnetwork: ${sys.get_env("VPC_SUBNETWORK_FULLNAME")}
                            noExternalIpAddress: true
                      instances:
                      - policy:
                          provisioningModel: SPOT
                          machineType: ${sys.get_env("MACHINE_TYPE")}
                    logsPolicy:
                      destination: CLOUD_LOGGING
            result: batchjob
        - result:
            return: ${batchjob}

  # Create a Kubernetes Job to process the video file inside the GKE cluster
  #
  schedule_k8s_job:
    params: [JOB_ID, BUCKET_NAME, FILE_NAME, PROJECT, LOCATION, OUTPUT_BUCKET_NAME, OUTPUT_PATH, CONTAINER_IMAGE_URL, GKE_CLUSTER_NAME, GKE_NAMESPACE]
    steps:
        - create_k8s_job:
            call: gke.create_job
            args:
              cluster_id: ${GKE_CLUSTER_NAME}
              namespace: ${GKE_NAMESPACE}
              project: ${PROJECT}
              location: ${LOCATION}
              job:
                apiVersion: "batch/v1"
                kind: "Job"
                metadata:
                  name: "${JOB_ID}"
                  # generateName: sample-job-pi
                  namespace: ${GKE_NAMESPACE}
                  annotations:
                    kueue.x-k8s.io/queue-name: lq-team-b
                    # kueue.x-k8s.io/priority-class: high-priority
                spec:
                  template:
                    metadata:
                      annotations:
                        gke-gcsfuse/volumes: "true"
                    spec:
                      serviceAccountName: k8s-sa-cluster
                      containers:
                      - name: transcoding
                        image: ${CONTAINER_IMAGE_URL}
                        command:
                          - "/bin/sh"
                          - "-c"
                          - ./entrypoint.sh
                        env:
                        - name: MEDIA
                          value: "${FILE_NAME}"
                        - name: OUTPUT_PATH
                          value: "/"
                        volumeMounts:
                        - name: gcs-fuse-input
                          mountPath: /input
                          readOnly: true
                        - name: gcs-fuse-output
                          mountPath: /output
                          readOnly: false
                        resources:
                            requests:
                                cpu: 16
                                memory: 64Gi
                      volumes:
                      - name: gcs-fuse-input
                        csi:
                          driver: gcsfuse.csi.storage.gke.io
                          volumeAttributes:
                            bucketName: "${BUCKET_NAME}"
                      - name: gcs-fuse-output
                        csi:
                          driver: gcsfuse.csi.storage.gke.io
                          volumeAttributes:
                            bucketName: "${OUTPUT_BUCKET_NAME}"
                            mountOptions: ${"only-dir=" + OUTPUT_PATH}
                      restartPolicy: Never
                  backoffLimit: 1
            result: gkejob
        - result:
            return: ${gkejob}
